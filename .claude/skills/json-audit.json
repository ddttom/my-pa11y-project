{
  "name": "json-audit",
  "description": "Comprehensive audit of results.json structure vs implementation and documentation",
  "version": "1.0.0",
  "trigger": {
    "type": "command",
    "command": "/json-audit"
  },
  "prompt": "You are performing a comprehensive \"ultrathink\" audit to catch data structure mismatches between results.json, implementation code, and documentation.\n\n# CRITICAL RULES\n\n1. ALWAYS read results.json FIRST before analyzing any code\n2. NEVER assume a field exists - verify in actual JSON\n3. Check BOTH that code writes data AND that it reads from correct location\n4. Verify metric collection happens BEFORE report generation uses it\n5. Cross-reference all documentation against actual implementation\n\n# SYSTEMATIC VERIFICATION WORKFLOW\n\n## Phase 1: Understand Actual Data Structure\n\n1. Read results.json from output directory\n2. Document all top-level keys using jq\n3. For each array (performanceAnalysis, contentAnalysis, seoScores, etc.):\n   - Get first element structure with jq\n   - Document all field names and types\n   - Note nested objects and their structures\n4. Create reference map of actual structure\n\n## Phase 2: Audit Report Generation Code\n\n1. Scan all files in src/utils/reportUtils/ for field references\n2. For each field access (e.g., `result.someField`, `m.someMetric`):\n   - Note file and line number\n   - Verify field exists in results.json structure\n   - Check if accessing array vs object correctly\n   - Verify nested property paths are correct\n3. Common problem patterns to check:\n   - `m.headingCount` when actual fields are `h1Count`, `h2Count`, `h3Count`\n   - Accessing array when it's an object (or vice versa)\n   - Nested properties like `m.llmsTxt.metrics.hasLLMsTxtReference`\n   - Non-existent computed fields like `m.servedScore`\n\n## Phase 3: Audit Metric Collection Code\n\n1. Check src/utils/metricsUpdater.js:\n   - List all `export function updateXxxMetrics` functions\n   - Verify each function is imported and called somewhere\n   - Check what data each function writes to results\n2. Check src/utils/urlProcessor.js:\n   - Verify all metric update functions are called\n   - Check calling order (some metrics may depend on others)\n3. Check src/utils/pageAnalyzer.js:\n   - Verify what data is collected and stored in pageData\n   - Check if pageData fields are used by metric updaters\n4. Report any metric collection functions that are defined but never called\n\n## Phase 4: Audit Executive Summary Code\n\n1. Read src/utils/reportUtils/executiveSummary.js completely\n2. For each `build*Summary` function:\n   - Document what fields it accesses from results\n   - Verify those fields exist in results.json\n   - Check for computed fields (servedScore, etc.) - verify computation is correct\n3. Check generateMarkdownSummary function:\n   - Verify all `summary.xxx` fields exist in summary object\n   - Check conditional rendering (if statements) use correct field names\n\n## Phase 5: Audit Technology Detection\n\n1. Check src/utils/technologyDetection.js:\n   - Verify it accesses results.externalResourcesAggregation\n2. Verify externalResourcesAggregation is populated:\n   - Check src/utils/metricsUpdater.js has updateExternalResourcesMetrics\n   - Check src/utils/urlProcessor.js calls updateExternalResourcesMetrics\n   - Verify function gets correct data (pageData.allResources)\n3. Check src/utils/pageAnalyzer.js:\n   - Verify it collects allResources or externalResources\n   - Check field names match what metricsUpdater expects\n\n## Phase 6: Audit Documentation\n\n1. Check docs/report-layout.md:\n   - Verify all documented fields exist in results.json\n   - Check field types (array vs object)\n   - Verify nested structure documentation\n2. Check CLAUDE.md:\n   - Verify \"Data Structures\" section is accurate\n   - Check example field names match actual structure\n3. Check CHANGELOG.md:\n   - Verify feature descriptions match actual implementation\n   - Check file references are accurate\n\n## Phase 7: Report Findings\n\n1. Create detailed report with:\n   - **CRITICAL**: Fields referenced but don't exist\n   - **CRITICAL**: Metric functions defined but never called\n   - **CRITICAL**: Wrong type access (array vs object)\n   - **HIGH**: Incorrect nested property paths\n   - **MEDIUM**: Documentation mismatches\n   - **LOW**: Potential improvements\n2. For each issue:\n   - File path and line number\n   - What code expects vs what actually exists\n   - Specific fix needed\n3. Provide actionable fix list:\n   - Fix priority (Critical first)\n   - Exact code changes needed\n   - Documentation updates needed\n\n# OUTPUT FORMAT\n\n```markdown\n# JSON Data Audit Report\n\n## Summary\n- Total issues found: X\n- Critical: X\n- High: X  \n- Medium: X\n- Low: X\n\n## Phase 1: Actual Data Structure\n[Document actual results.json structure]\n\n## Phase 2: Report Generation Issues\n[List all field access mismatches]\n\n## Phase 3: Metric Collection Issues\n[List uncalled functions or missing calls]\n\n## Phase 4: Executive Summary Issues\n[List field mismatches in executive summary]\n\n## Phase 5: Technology Detection Issues\n[List issues with tech detection data flow]\n\n## Phase 6: Documentation Issues\n[List documentation mismatches]\n\n## Phase 7: Fix Action Plan\n[Prioritized list of fixes with exact changes needed]\n```\n\n# TOOLS AVAILABLE\n\n- Read: Read any source file\n- Bash: Run jq commands on results.json\n- Grep: Search for field references across codebase\n- Glob: Find all report generation files\n\n# EXECUTION\n\nExecute all 7 phases systematically. Do NOT skip any phase. Do NOT assume anything - verify everything against actual files.",
  "tools": ["Read", "Bash", "Grep", "Glob"],
  "model": "sonnet",
  "user_invocable": true
}
